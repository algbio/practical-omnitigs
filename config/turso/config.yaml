use-conda: true
cluster: >-
  sbatch
  --job-name=practical-omnitigs:{rule}:$(sed "s\/\_\g" <<< '{wildcards}' | sed "s/ /_/g" | head -c 100)
  --chdir=/proj/sebschmi/git/practical-omnitigs
  -o "$(cat .logdir)/practical-omnitigs:{rule}:$(sed "s:[\\/ Â¤]:_:g" <<< '{wildcards}' | sed "s/ /_/g" | head -c 100):%j.log"
  -p {resources.queue}
  -c {resources.cpus}
  -t {resources.time_min}
  -M {resources.cluster}
  --mem={resources.mem_mb}
  --constraint="{resources.cluster_constraint}"
  --mail-type={resources.mail_type}
  --mail-user=sebastian.schmidt@helsinki.fi
  --signal=SIGINT@300
  {resources.options}
#  --hold
#  $(./scripts/parse_sbatch_job_id.sh {dependencies})
cluster-status: "scripts/get_slurm_job_status.py"
max-status-checks-per-second: 10
cluster-cancel: "scripts/cancel_slurm_jobs.py"
cluster-cancel-nargs: 1000
default-resources: [cpus=1, time_min=60, mem_mb=100, queue="short,medium,long,aurinko", mail_type=FAIL, cluster=ukko, cluster_constraint=intel, options="", disk_mb=10000000]
rerun-incomplete: true
#immediate-submit: true
#notemp: true
printshellcmds: true
jobs: 100
cores: 10000
local-cores: 8
latency-wait: 60
config: "datadir=/wrk-vakka/users/sebschmi/practical-omnitigs/data"
keep-going: true
scheduler: "greedy" # ILP creates files in bad locations, possibly leading to a lockup of the file system
shadow-prefix: "/wrk-vakka/users/sebschmi/practical-omnitigs"
